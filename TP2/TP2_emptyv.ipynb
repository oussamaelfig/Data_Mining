{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DqQpOBTiDzDG"
   },
   "source": [
    "# INF8111 - Fouille de données / Data Mining\n",
    "## Été 2024 - TP2 - Fouille de réseaux sociaux / Mining of social networks\n",
    "### Membres de l'équipe / Team members\n",
    "    - Name1 - Student Id - 1\n",
    "    - Name2 - Student Id - 2\n",
    "    - Name3 - Student Id - 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zqh6ruHXDzDK"
   },
   "source": [
    "## Instructions de remise / Submission\n",
    "\n",
    "Vous devez remettre dans la boîte de remise sur moodle:\n",
    "\n",
    "1. ce fichier nommé TP2\\_NomDuMembre1\\_NomDuMembre2\\_NomDuMembre3.ipynb\n",
    "\n",
    "**N.B**: Assurez-vous que tous les résultats soient lisibles lorsque le notebook est ouvert.\n",
    "\n",
    "Ce notebook être remis avant le **02 juin 2024 à 23h55**. Tout travail en retard sera pénalisé d’une valeur de 10\\% par jour ouvrable de retard.\n",
    "\n",
    "## Barème\n",
    "\n",
    "Partie 1: 10 points\n",
    "\n",
    "Partie 2: 6 points\n",
    "\n",
    "Partie 3: 4 points\n",
    "\n",
    "Pour un total de 20 points.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Submission\n",
    "\n",
    "You must put back in the submission box on moodle:\n",
    "\n",
    "1. this file renamed TP2\\_NomDuMembre1\\_NomDuMembre2\\_NomDuMembre3.ipynb\n",
    "\n",
    "**N.B**: Make sure that all results are there when you open your notebook.\n",
    "\n",
    "Everything must be submitted before **June 2nd 2024 à 23h55**. Any late work will be penalized with a value of 10% per open day of delay.\n",
    "\n",
    "## Barème\n",
    "Part 1: 10 points\n",
    "\n",
    "Part 2: 6 points\n",
    "\n",
    "Part 3: 4 points\n",
    "\n",
    "For a total of 20 points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1AIPBwaDzDL"
   },
   "source": [
    "## Réseaux sociaux / Social Networks\n",
    "Les réseaux sociaux occupent une grande partie de nos vies. Tout au long de sa,vie, chaque personne appartient à différentes communautés. Avec la récupération de ces informations par les différents réseaux sociaux, les data-analystes ont voulu exploiter ces données. C'est un domaine relativement nouveau qui est en pleine croissance avec de nombreux impacts, notamment sur la publicité et les systèmes de recommandation. \n",
    "\n",
    "### But\n",
    "Le but de ce TP est de vous donner un aperçu de l'analyse de réseau social.\n",
    "\n",
    "Dans la première partie, vous implémenterez un algorithme de détection de communautés dans un réseau social nommé LPAm+. Cet algorithme a été proposé par [X. Liu et T. Murata en 2010](https://www.sciencedirect.com/science/article/pii/S0378437109010152).\n",
    "\n",
    "Dans la deuxième partie, vous trouverez les personnes avec le plus d'influence dans leur réseau social. \n",
    "\n",
    "Pour les deux parties, nous vous fournissons les CSV contenant les réseaux sociaux à analyser.\n",
    "\n",
    "---\n",
    "\n",
    "## Social networks\n",
    "Social networks are a major component of the human life. Each person belongs throughout their life to different communities. With the aggregation of information on various online social media platforms, data analysts were interested in exploiting its data. It is a relatively new field that is growing with impacts on several aspects such as advertising and recommendation systems.\n",
    "\n",
    "\n",
    "### Goal\n",
    "The purpose of this lab is to give you an overview of social network analysis.\n",
    "\n",
    "In the first part, you will implement an algorithm for detecting communities in a social network called LPAm+. This algorithm was proposed by [X. Liu and T. Murata in 2010](https://www.sciencedirect.com/science/article/pii/S0378437109010152).\n",
    "\n",
    "In the second part, you will find the people with the most influence in their social network.\n",
    "\n",
    "For both parts, we provide you with the CSV containing the social networks to be analysed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ww_wy2y2DzDM"
   },
   "source": [
    "# 1. LPAm+ (10 points)\n",
    "\n",
    "## Détection de communauté\n",
    "La détection de communauté dans un réseau social est une manipulation fréquente lors de l'analyse d'un réseau. Une méthode de clustering est utilisée pour regrouper les personnes dans des communautés selon leurs liens. \n",
    "\n",
    "## LPAm+\n",
    "Dans cette partie, vous devez implémenter l'algorithme LPAm+ pour détecter les communautés parmi les personnages de l'univers cinématographique Marvel (MCU, en anglais). Vous devez utiliser les CSVs *nodes* et *edges* pour cela. \n",
    "\n",
    "Cet algorithme consiste à propager les étiquettes dans le réseau selon une règle d'évaluation optimisant la modularité du réseau. Lorsque l'algorithme atteint un optimum local, il regarde s'il peut fusionner deux communautés pour augmenter la modularité du réseau. L'algorithme choisit toujours la combinaison la plus avantageuse. Si une combinaison est trouvée, la propagation des étiquettes est refaite. L'algorithme continue tant qu'il peut améliorer la modularité. Vous pouvez lire l'article mentionné plus haut pour plus de détails, mais cela n'est pas nécessaire puisque vous allez être guidé tout le long du TP. \n",
    "\n",
    "Pour faciliter la représentation du réseau, nous vous proposons d'utiliser la librairie networkx. La documentation est disponible [ici](https://networkx.github.io/documentation/stable/tutorial.html).\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Community detection\n",
    "Community detection in a social network is a frequent manipulation when analysing a network. A clustering method is used to bring people together in communities according to the links between them.\n",
    "\n",
    "\n",
    "## LPAm+\n",
    "In this part, you will implement the LPAm+ algorithm to detect the communities among the characters of the Marvel Cinematic Universe (MCU). You must use the nodes and edges csv for this.\n",
    "\n",
    "This algorithm consists in propagating the labels in the network according to an evaluation rule optimizing the modularity of the network. When the algorithm reaches a local optimum, it checks whether it can combine two communities to increase the modularity of the network. The algorithm always chooses the most advantageous combination. If a combination is found, the propagation of the labels is redone. The algorithm continues until it is no longer able to increase modularity. You can read the article mentioned above for more details, but you do not need to, as you will be guided throughout the TP.\n",
    "\n",
    "\n",
    "To help you represent a network, we suggest that you use the networkx package.You can read more about the package [here](https://networkx.github.io/documentation/stable/tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1re6f2R8DzDN"
   },
   "outputs": [],
   "source": [
    "# vous pouvez bien sûr utiliser anaconda pour installer les packages\n",
    "\n",
    "!pip install --user numpy\n",
    "!pip install --user pandas\n",
    "!pip install --user matplotlib\n",
    "!pip install --user networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ougnwqizDzDP"
   },
   "outputs": [],
   "source": [
    "# vérifier si les libraries sont bien installés\n",
    "\n",
    "import networkx\n",
    "print('networkx: {}'.format(networkx.__version__))\n",
    "\n",
    "import matplotlib\n",
    "print('matplotlib: {}'.format(matplotlib.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jsBjqzWiDzDQ"
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import random\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "class LPAmPlus:\n",
    "    \"\"\"\n",
    "    Constructor\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    def __init__(self, graph):\n",
    "        \"\"\"\n",
    "        graph gives the graph on which the algorithm will be applied;\n",
    "        \"\"\"\n",
    "        #self.graph = graph\n",
    "\n",
    "        ## TODO\n",
    "        \n",
    "        \"\"\"\n",
    "        labels gives all the communities present in the network\n",
    "        \"\"\"\n",
    "        #self.labels = None   \n",
    "\n",
    "        ## TODO         \n",
    "\n",
    "        \"\"\"\n",
    "        Assign a label to each node\n",
    "        \"\"\"\n",
    "        ## TODO\n",
    "\n",
    "    \"\"\"\n",
    "    Term to optimize when replacing labels\n",
    "    \"\"\"\n",
    "\n",
    "    def label_evaluation(self, current_node, new_label):\n",
    "        \n",
    "        ## TODO\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Function to choose the new label for a node\n",
    "    \"\"\"\n",
    "\n",
    "    def update_label(self, current_node):\n",
    "       \n",
    "        ## TODO\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Function that calculates the current modularity of the network\n",
    "    \"\"\"\n",
    "\n",
    "    def modularity(self):\n",
    "        \n",
    "        ## TODO\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Function that applies the LPAm algorithm on the network\n",
    "    \"\"\"\n",
    "\n",
    "    def LPAm(self):\n",
    "        \n",
    "        ## TODO\n",
    "\n",
    "\n",
    "   \n",
    "    \"\"\"\n",
    "    Function that find which communities to combine and combine them\n",
    "    \"\"\"\n",
    "    def merge_communities(self):\n",
    "        \n",
    "        ## TODO\n",
    "\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Function that applies the LPAm+ algorithm on the network\n",
    "    \"\"\"\n",
    "\n",
    "    def find_communities(self):\n",
    "        ## TODO\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPHv-JEnDzDR"
   },
   "source": [
    "### 1.1 Dataset (1 point)\n",
    "\n",
    "Nous vous avons fourni le fichier *`tp-marvel-dataset-subset.csv`* contenant l'ensemble des personnages de l'univers cinématographique Marvel. Vous devez maintenant créer un graphe reliant tous les personnages. Vous devez suivre les règles ci-dessous :\n",
    "\n",
    "#### Mise en œuvre\n",
    "1. Implémentez la fonction *`load_unweighted_network`*. Cette fonction retourne un graphe non orienté et non pondéré.\n",
    "2. Chaque personnage est un nœud unique du graphe. Aucun personnage ne peut donc apparaître deux fois dans le graphe.\n",
    "3. Tous les personnages d'un même film doivent être connectés.\n",
    "\n",
    "Utilisez la fonction `test_load` pour vérifier votre implémentation de la fonction. Ce test utilise un sous-ensemble de l'ensemble de données. Vous devriez obtenir un résultat semblable à celui-ci :\n",
    "\n",
    "![titre](picture.png)\n",
    "\n",
    "---\n",
    "We have provided you with the a .CSV file, called *`tp-marvel-dataset-subset.csv`*, with all the characters from movies in the Marvel Cinematic Universe. You must now create a graph connecting all the characters. You should follow the rules below:\n",
    "\n",
    "#### Implementation\n",
    "1. Implement the function *`load_unweighted_network`*. This function returns a undirected and unweighted graph.\n",
    "2. Each character must be a single node inside the graph. No character should appear twice in the graph. \n",
    "3. All characters in a same movie should be connected.\n",
    "\n",
    "Use the function `test_load` to verify your implementation of the function. This test uses a subset of the whole dataset. You should obtain a result similar to this:\n",
    "\n",
    "![title](picture.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gEFqXHojDzDR"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "def load_unweighted_network(source_csv):\n",
    "    ## TODO\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2QFS4CmDzDS"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def test_load():\n",
    "    network = load_unweighted_network(\"data/tp-marvel-dataset-subset.csv\")\n",
    "    nx.draw_networkx(network, node_size=60, font_size=10, edge_color = 'gray', font_color='black')\n",
    "    plt.figure(figsize=(20,20))\n",
    "    plt.show()\n",
    "\n",
    "test_load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tagfYdSYDzDT"
   },
   "source": [
    "### 1.2  Modularité / Modularity (1 point)\n",
    "\n",
    "La modularité $Q$ du réseau est une mesure importante pour l'algorithme: elle permet de savoir si l'algorithme a atteint un optimum local. $$ Q=\\frac{1}{2m}\\sum_{u,v=1}^n B_{uv}\\delta(l_u,l_v)$$ \n",
    "\n",
    "- m: nombre d'arêtes\n",
    "- l: étiquette d'un sommet\n",
    "- u, v: sommets du réseau\n",
    "- B: matrice de modularité (chaque élément vaut $A_{uv} - P_{uv}$)\n",
    "- $A_{uv}$: vaut 1 si il y une arête entre u et v sinon 0\n",
    "- $P_{uv}$: la probabilité qu'il y ait une arête entre u et v selon le modèle nul  $$P_{uv}=\\frac{degree(u)*degree(v)}{2m}$$\n",
    "- $\\delta(l_u,l_v)$: symbole de Kronecker, vaut 1 si les deux labels sont identiques sinon 0\n",
    "\n",
    "#### Implémentation\n",
    "1. Implémentez  la fonction  `modularity`  dans LPAmPlus. Cette fonction retourne la modularité du réseau. Vous pouvez utiliser la fonction `nx.linalg.modularity_matrix` de networkx pour calculer la matrice B. **N.B:** Networkx permet d'ajouter du data sur les sommets pour garder des informations sur le node. Les `nodes` agissent comme des dictionnaires.\n",
    "\n",
    "Utilisez la fonction `test_modularity` pour vérifier votre implémentation de la fonction. Vous devriez obtenir une modularité d'environ 0.019.\n",
    "\n",
    "---\n",
    "\n",
    "The modularity $Q$ of the network is an important measure for the algorithm. The algorithm uses it to determine if it reached a local optimum or not. $$ Q=\\frac{1}{2m}\\sum_{u,v=1}^n B_{uv}\\delta(l_u,l_v)$$ \n",
    "\n",
    "- m: number of edges\n",
    "- l: node's label\n",
    "- u, v: nodes in the graph\n",
    "- B: modularity matrix where each element is $A_{uv} - P_{uv}$\n",
    "- $A_{uv}$: is 1 if there is an edge between u and v else 0\n",
    "- $P_{uv}$: probability that there is an edge between u and v following the null model $$P_{uv}=\\frac{degree(u)*degree(v)}{2m}$$\n",
    "- $\\delta(l_u,l_v)$: Kronecker's delta, is 1 if labels are the same else 0\n",
    "\n",
    "#### Implementation\n",
    "1. Implement the function `modularity` in the class LPAmPlus. This function returns the modularity of the network. You can use the function `nx.linalg.modularity_matrix` from networkx to calculate B. **N.B:** You can add data to nodes with Networkx to store information about the node. You can add data to nodes with Networkx to store information about the node. The nodes act like a dictionnary.\n",
    "\n",
    "Use the function `test_modularity` to test your implementation. You should have a modularity of 0.019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k1JIWRufDzDT"
   },
   "outputs": [],
   "source": [
    "def test_modularity():\n",
    "    social_network = load_unweighted_network(\"data/tp-marvel-dataset-subset.csv\")\n",
    "    lpam = LPAmPlus(social_network)\n",
    "    lpam.labels = [0, 1]\n",
    "    for i in range(0, 16):\n",
    "        lpam.graph.nodes[i]['label'] = 0 if i < 10 else 1\n",
    "    print(\"Modularity: {:.3f}\".format(lpam.modularity()))\n",
    "\n",
    "test_modularity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lflryGWyDzDT"
   },
   "source": [
    "### 1.3 Règle de modification des étiquettes / Updating rule for the labels (2 points)\n",
    "\n",
    "Comme mentionné plus haut, l'algorithme est basé sur une optimisation de la modularité. Il vous est demandé d'implémenter le terme à optimiser. La nouvelle étiquette $l_x^{new}$ correspond à l'étiquette pour laquelle la somme a la plus grande valeur.\n",
    "$$l_x^{new}=\\arg\\max_l\\sum_{u=1}^n B_{ux}\\delta(l_u,l)$$\n",
    "\n",
    "- n: le nombre de sommets\n",
    "- m: le nombre d'arêtes\n",
    "- l: une étiquette possible pour le sommet x\n",
    "- x: le sommet qu'on évalue en ce moment\n",
    "- u: un autre sommet dans le réseau (commence à 1, car on exclut le sommet x)\n",
    "- B: la matrice de modularité où chaque élément vaut $A_{ux} - P_{ux}$\n",
    "- $A_{ux}$: vaut 1 si il y une arête entre u et x sinon 0\n",
    "- $P_{ux}$: la probabilité qu'il y ait une arête entre u et x selon le modèle nul  $$P_{ux}=\\frac{degree(u)*degree(x)}{2m}$$\n",
    "- $\\delta(l_u,l)$: delta de Kronecker, vaut 1 si les deux labels sont identiques sinon 0\n",
    "\n",
    "\n",
    "#### Implémentation\n",
    "1. Implémenter la fonction `label_evaluation`. Cette fonction retourne la valeur du terme à optimiser. Vous pouvez utiliser la fonction `linalg.modularity_matrix` de networkx pour calculer la matrice B. Il est normal qu'il y ait une ressemblance avec le calcul de la modularité selon la définition que vous avez prise. `new_label` correspond donc à un $l$ possible dans le terme.\n",
    "2. Implémenter la fonction `update_label`. Cette fonction met à jour la nouvelle étiquette pour un sommet. En cas d'égalité, la fonction choisit une étiquette au hasard parmi les meilleurs. N'oubliez pas d'enlever les étiquettes désuètes du paramètre `labels`. **N.B:** Il est possible que la meilleure étiquette soit celle actuelle du sommet.\n",
    "\n",
    "Networkx permet d'ajouter du data sur les sommets. Les sommets sont des dictionnaires dans le graphe.\n",
    "\n",
    "---\n",
    "\n",
    "As mentioned above, the algorithm is strongly based on its optimization of modularity. You are now asked to implement the term to optimize. The new label $l_x^{new}$ corresponds to the label for which the sum gives the greatest value.\n",
    "$$l_x^{new}=\\arg\\max_l\\sum_{u=1}^n B_{ux}\\delta(l_u,l)$$\n",
    "\n",
    "- n: number of nodes\n",
    "- m: number of edges\n",
    "- l: a possible label for the node x\n",
    "- x: current node being evaluated\n",
    "- u: another node in the network (starts at 1, because we exclude the node x)\n",
    "- B: modularity matrix where each element is $A_{ux} - P_{ux}$\n",
    "- $A_{ux}$: is 1 if there is an edge between u and x else 0\n",
    "- $P_{ux}$: the probability that there is an edge between u and x  following the null model  $$P_{ux}=\\frac{degree(u)*degree(x)}{2m}$$\n",
    "- $\\delta(l_u,l)$: Kronecker's delta, is 1 if labels are the same else 0\n",
    "\n",
    "\n",
    "#### Implementation\n",
    "1. Implement the function `label_evaluation`. This function returns the value for the term to optimize. You can use the function `linalg.modularity_matrix` from networkx to calculate B. It is normal if there is a similarity with the modularity depending on the definition you took. `new_label` represent a possible $l$ in the term.\n",
    "2. Implement the function `update_label`. This function chooses the new label for the current node. If there is more than one label with the max value, the function chooses randomly one amoung those. Don't forget to remove the unused labels from the `labels` attribute. **N.B:** The best label can be the node's current label. \n",
    "\n",
    "You can add data to nodes with Networkx to store information about the node. The nodes act like a dictionnary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bceEw3PvDzDU"
   },
   "source": [
    "### 1.4 LPAm (2 points)\n",
    "\n",
    "Vous pouvez maintenant implémenter l'algorithme LPAm. Cet algorithme est le prédécesseur de LPAm+ puisque LPAm+ a été crée pour contourner une faiblesse de LPAm.  LPAm est un algorithme de propapagation d'étiquettes basé sur la modularité. Il commence par donner une étiquette unique à chaque sommet. Il explore par la suite tous les sommets et modifie leur étiquette selon la fonction d'évaluation que vous avez implémentée plus tôt. L'algorithme continue la propagation d'étiquette à travers tous les sommets jusqu'à un optimum de la modularité.\n",
    "\n",
    "#### Implémentation\n",
    "1. Ajouter les étiquettes initiales aux sommets du graphe avec la fonction `__init__`. Il faut que chaque sommet soit dans sa propre communauté au début de l'algorithme. Initialiser le paramètre `labels` pour qu'il contient la liste des étiquettes présentes dans le réseau.\n",
    "\n",
    "2. Implémenter l'algorithme LPAm dans la fonction `LPAm`. Assurez-vous de toujours augmenter la modularité lors de vos changements d'étiquettes. N'oubliez pas de garder le paramètre `labels` à jour à fur et à mesure lors de vos changements pour ne pas évaluer plusieurs fois la même étiquette.\n",
    "\n",
    "Utilisez la fonction `test_lpam` pour vérifier votre implémentation. Vous devriez finir avec une modularité d'environ 0.184 avec 5 communautés.\n",
    "\n",
    "---\n",
    "\n",
    "You can now implement the LPAm algorithm. This algorithm is the predecessor of LPAm+ since LPAm+ was created to overcome LPAm's weakness. LPAm is a label probagation algorithm based on modularity. It begins by giving a unique label to each node. It then explores all the nodes and changes their label according to the evaluation function that you implemented earlier. The algorithm continues until it can no longer improve the modularity of the network.\n",
    "\n",
    "#### Implementation\n",
    "1. Add the initial labels to the nodes in the graph in the function `__init__`. Each nodes has to be in their own community in the beginning. Initialise `labels` with the current list of labels present in the graph.\n",
    "\n",
    "2. Implement the LPAm algorithm in the function`LPAm`. Make sure that all your labels changes improve the modularity. Don't forget to keep your `labels` parameter is kept up-to-date so that you dont evaluate the same label multiple times or unused labels.\n",
    "\n",
    "Use the function `test_lpam` to verify your implementation. You should have a modularity of 0.184 with 5 communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WwOv71SBDzDU"
   },
   "outputs": [],
   "source": [
    "def test_lpam():\n",
    "    social_network = load_unweighted_network(\"data/tp-marvel-dataset-subset.csv\")\n",
    "    lpam = LPAmPlus(social_network)\n",
    "    lpam.LPAm()\n",
    "    print(\"Modularity: {:.3f}\\nCommunities: {}\".format(lpam.modularity(), lpam.labels))\n",
    "\n",
    "test_lpam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9E9ud6rDzDU"
   },
   "source": [
    "### 1.5 LPAm+ (2 points)\n",
    "\n",
    "Vous pouvez maintenant implémenter LPAm+ au complet. LPAm+ est une amélioration de LPAm. Lorsque LPAm tombe dans un optimum local, LPAm+ essaye de combiner deux communautés pour augmenter la modularité et ainsi sortir du optimum local. LPAm+ choisit la combinaison qui augmente le plus la modularité et recommence la propagation d'étiquette jusqu'au prochain optimum local où il va réessayer de combiner des communautés. L'algorithme continue jusqu'à qu'il ne puisse plus augmenter la modularité.\n",
    "\n",
    "#### Implémentation\n",
    "1. Implémentez  la fonction  `merge_communities`. Cette fonction évalue si combiner des communautés augmente la modularité et combine le meilleur choix. Elle retourne True si une combinaison a été faite sinon False (aucune combinaison augmente la modularité).\n",
    "2. Implémenter `find_communities`. Cette fonction applique l'algorithme LPAm+ sur le réseau en utilisant les fonctions `LPAm` et `merge_communities`.\n",
    "\n",
    "Utilisez la fonction `test_lpam_plus` pour vérifier votre implémentation. Vous devriez finir avec une modularité d'environ 0.204 et 3 communautés.\n",
    "\n",
    "---\n",
    "\n",
    "You can now fully implement LPAm+. As said before LPAm+ is an amelioration of LPAm. The issue with LPAm is that it stops when it finds a local optimun. To prevent that, LPAm+ tries to combine two communities to increase modularity and escape the local optimun. LPAm+ chooses the combination that most increases modularity and restart the label's propagation until the next local optimum where it will try to combine two communities again. The algorithm continues until it can no longer increase modularity.\n",
    "\n",
    "#### Implementation\n",
    "1. Implement the function  `merge_communities`. This function check if combining communities improve the modularity and combine the best choice. It returns True if a combinaison was made else False (no combination increase the modularity).\n",
    "2. Implement the LPAM+ algorithm in the function `find_communities` using the fonctions `LPam` and `merge_communities`.\n",
    "\n",
    "Use the function `test_lpam_plus` to verify your implementation. You should end with a modularity of 0.204 and 3 communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blXGH-woDzDV"
   },
   "outputs": [],
   "source": [
    "def test_lpam_plus():\n",
    "    social_network = load_unweighted_network(\"data/tp-marvel-dataset-subset.csv\")\n",
    "    lpam = LPAmPlus(social_network)\n",
    "    lpam.find_communities()\n",
    "    print(\"Modularity: {:.3f}\\nCommunities: {}\".format(lpam.modularity(), lpam.labels))\n",
    "\n",
    "test_lpam_plus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8srl1uFDzDV"
   },
   "source": [
    "### 1.6 MCU dataset (2 points)\n",
    "\n",
    "Exécutez votre algorithme sur l'ensemble de données de caractères MCU complet (*`tp-marvel-dataset.csv`*) et comparez ce que vous obtenez avec les communautés réelles. Ces communautés réelles se trouvent dans la colonne Affiliation du csv.\n",
    "\n",
    "Commencez par calculer le RI (Rand index) de vos résultats. $$ RI=\\frac{TP+TN}{TP+TN+FP+FN} = \\frac{TP+TN}{\\binom{n}{2}}$$\n",
    "\n",
    "- n : nombre de nœuds\n",
    "- TP : Vrai positif le nombre de paires d'éléments qui sont dans la même communauté dans vos résultats et dans la vérité terrain\n",
    "- TN : Vrai négatif le nombre de paires d'éléments qui se trouvent dans des communautés différentes dans vos résultats et dans la vérité terrain\n",
    "- FP : Faux positif le nombre de paires d'éléments qui sont dans la même communauté dans vos résultats mais qui sont dans des communautés différentes dans la vérité terrain\n",
    "- FN : Faux négatif le nombre de paires d'éléments qui sont dans des communautés différentes dans vos résultats mais qui sont dans la même communauté dans la vérité terrain\n",
    "\n",
    "**N.B :** Ce qui compte ici, c'est la composition des communautés que vous avez trouvées, pas les noms. Un TP, c'est quand le nœud a et le nœud b sont tous les deux dans les mêmes communautés dans votre résultat et dans la vérité terrain.\n",
    "\n",
    "Répondre aux questions suivantes. Ce sont des guides pour votre analyse.\n",
    "\n",
    "- L'algorithme fonctionne-t-il bien ?\n",
    "- Le manque de nombreuses communautés affecte-t-il les performances algorithmiques ?\n",
    "- Comment le fait de connecter tous les personnages d'un film affecte-t-il les résultats ?\n",
    "- Expliquez pourquoi vous avez obtenu ces résultats en analysant les communautés dans l'ensemble de données. Quelles particularités offrent de meilleurs résultats ou entravent l'algorithme ?\n",
    "\n",
    "Vous pouvez faire les manipulations que vous souhaitez pour mieux présenter vos résultats et mieux étayer vos propos.\n",
    "\n",
    "---\n",
    "\n",
    "Run your algorithm over whole MCU characters dataset (*`tp-marvel-dataset.csv`*) and compare what you get and the real communities. The ground truth is found in the Affiliation column in the csv. \n",
    "\n",
    "Start by calculating the RI (Rand index) of your results. $$ RI=\\frac{TP+TN}{TP+TN+FP+FN} = \\frac{TP+TN}{\\binom{n}{2}}$$\n",
    "\n",
    "- n: number of nodes\n",
    "- TP: True positive the number of pairs of elements that are in the same community in your results and in the ground truth\n",
    "- TN: True negative the number of pairs of elements that are in different communities in your results and in the ground truth\n",
    "- FP: False positive the number of pairs of elements which are in the same community in your results but which are in different communities in the ground truth\n",
    "- FN: False negative the number of pairs of elements which are in different communities in your results but which are in the same community in the ground truth\n",
    "\n",
    "**N.B:** What matters here is the composition of the communities you found not the names. A TP is when the node a and the node b are both in the same communities in your result and in the ground truth.\n",
    "\n",
    "Answer the following questions. They are guides for your analysis.\n",
    "\n",
    "- Does the algorithm perform well?\n",
    "- Does the lack of many communities affect the algorithmic performance?\n",
    "- How does the fact of connecting all characters in a movie affect the results?\n",
    "- Explain why you obtained those results by analysing the communities in the dataset. Which particularities offer better results, or hinder the algorithm?\n",
    "\n",
    "You can do the manipulations you want to better present your results and better support your statements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1y0vz2wDzDV"
   },
   "source": [
    "#### Résultats / Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iteF1Me9DzDW"
   },
   "outputs": [],
   "source": [
    "## Mettez votre code ici\n",
    "\n",
    "## Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ee4mLl8FDzDW"
   },
   "source": [
    "#### Analyse / Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h42s5mBIDzDW"
   },
   "outputs": [],
   "source": [
    "## Écrivez ici\n",
    "\n",
    "## Write here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDxuPJq2DzDW"
   },
   "source": [
    "# 2. Social network analysis (6 points)\n",
    "\n",
    "Une autre analyse intéressante à faire avec un réseau social est de trouver les personnes influentes du réseau, c'est-à-dire les personnes autour desquelles les personnes du réseau se regroupent.\n",
    "\n",
    "Il y a des mesures qui permettent de connaître ces personnes : les mesures de centralité. **Vous devez implémenter ces métriques vous-mêmes. N'utilisez pas l'implémentation `networkx` pour le tp.** Utilisez le dataset Marvel complet. \n",
    "\n",
    "---\n",
    "\n",
    "Another interesting analysis to do with a social network is to find the influential people in the network, ie the people around whom the people in the network gather.\n",
    "\n",
    "There are measures which make it possible to know these people: the centrality measures. **You must implement those metrics yourselves. Do not use `networkx` implementation for the  tp.** Use the whole Marvel dataset. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15zJrLivDzDW"
   },
   "source": [
    "## 2.1 Centralité de degré / Degree centrality (2 points)\n",
    "\n",
    "Une première mesure simple pour trouver l'importance d'un sommet dans un réseau est la centralité de degré. Elle se calcule selon la formule suivante $$C_{D}(i) = \\frac{degree(i)}{n-1}$$\n",
    "\n",
    "- i: un sommet dans le réseau\n",
    "- n: le nombre de sommets\n",
    "- degree: le nombre d'arêtes attachées au sommet\n",
    "\n",
    "#### Implémentation\n",
    "1. Implémenter la fonction `calculate_degree_centrality`. Cette fonction calcule la centralité de degré pour tous les sommets du réseau et ajoute cette mesure à chaque sommet.\n",
    "\n",
    "2. Affichez les 10 centralités les plus élevées pour l'ensemble de données MCU. À titre indicatif, la centralité la plus élevée devrait être de 0.48.\n",
    "\n",
    "---\n",
    "\n",
    "A first simple measure to find the importance of a node in a network is the degree centrality. It is calculated $$C_{D}(i) = \\frac{degree(i)}{n-1}$$\n",
    "\n",
    "- i: a node in the network\n",
    "- n: the number of nodes\n",
    "- degree: the number of edges attached to the node\n",
    "\n",
    "#### Implementation\n",
    "1. Implement the function `calculate_degree_centrality`. This function calculates degree centrality for all nodes in the network and adds this measurement to each node.\n",
    "2. Show the highest 10 centralities for the MCU dataset. As a hint, the highest centrality should be 0.48."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iSlHIvjoDzDX"
   },
   "outputs": [],
   "source": [
    "def calculate_degree_centrality(social_network):\n",
    "    ## TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YATDDlcmDzDX"
   },
   "source": [
    "## 2.2 Centralité de proximité / Closeness centrality (2 points)\n",
    "\n",
    "Une autre mesure simple pour trouver l'importance d'un sommet dans un réseau est la centralité de proximité. Elle se calcule avec la formule suivante $$C_{P}(i) = \\frac{1}{AvDist(i)}$$\n",
    "\n",
    "- i: un sommet dans le réseau\n",
    "- AvDist: la moyenne de toutes les distances les plus courtes pour atteindre chaque sommet à partir du sommet i\n",
    "\n",
    "#### Implémentation\n",
    "1. Implémenter la fonction `calculate_closeness_centrality`. Cette fonction calcule la centralité de proximité pour tous les sommets du réseau et ajoute cette mesure à chaque sommet. Considérer chaque arête comme une distance de 1.\n",
    "\n",
    "**NB**: Utiliser la fonction `shortest_path()` du module Networkx pour trouver le chemin le plus court entre des sommets\n",
    "\n",
    "2. L'ensemble de données a deux composants connectés. Utilisez la fonction `nx.connected_components` pour obtenir chacun d'eux. Calculez la centralité de proximité pour chaque composante et expliquez vos résultats.\n",
    "\n",
    "3. Affichez les 10 centralités les plus élevées pour le plus grand composant connexe. À titre indicatif, la centralité la plus élevée devrait être de 0.67.\n",
    "\n",
    "---\n",
    "\n",
    "Another simple measure for finding the importance of a node in a network is closeness centrality. It is calculated $$C_{P}(i) = \\frac{1}{AvDist(i)}$$\n",
    "\n",
    "- i: a node in the network\n",
    "- AvDist: the average of all shortest distances to reach each vertex from vertex i\n",
    "\n",
    "#### Implementation\n",
    "1. Implement the function `calculate_closeness_centrality`. This function calculates closeness centrality for all nodes in the network and adds this measurement to each node. Consider each edge as a distance of 1.\n",
    "\n",
    "**NB**: Use the fucntion `shortest_path()` from Networkx to find the shortest path between two nodes.\n",
    "\n",
    "2. The dataset has two connected components. Use the function `nx.connected_components` to get each one of them. Calculate the closeness centrality for each component, and explain your results. \n",
    "\n",
    "3. Show the highest 10 centralities for the largest connected component. As a hint, the highest centrality should be 0.67."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFOOFuovDzDX"
   },
   "outputs": [],
   "source": [
    "def calculate_closeness_centrality(social_network):\n",
    "  ## TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIyAQVj4DzDX"
   },
   "source": [
    "## 2.3 Centralité d'intermédiarité / Betweeness centrality (2 points)\n",
    "\n",
    "Une dernière mesure simple pour trouver l'importance d'un sommet dans un réseau est la centralité d'intermédiarité. Elle se calcule avec cette formule $$C_{I}(i) = \\frac{\\sum_{j<k}f_{jk}(i)}{\\binom{n-1}{2}}$$\n",
    "\n",
    "- n: le nombre de sommets dans le réseau\n",
    "- i: un sommet dans le réseau\n",
    "- j,k: deux sommets dans le réseau excluant i\n",
    "- $f_{jk}(i)$: la proportion des chemin le plus court partant du sommet j vers un sommet k (> j) passant par le sommet i \n",
    "\n",
    "#### Implémentation\n",
    "1. Implémenter la fonction `calculate_betweenness_centrality`. Cette fonction calcule la centralité d'intermédiarité pour tous les sommets du réseau et ajoute cette mesure à chaque sommet.\n",
    "\n",
    "**NB**: Utilisez les fonctions `all_shortest_paths()` ou `shortest_path()` de Networkx pour calculer des chemins simples les plus courts du graphe.\n",
    "\n",
    "2. L'ensemble de données a deux composants connectés. Utilisez la fonction `nx.connected_components` pour obtenir chacun d'eux. Calculez la centralité de proximité pour chaque composante et expliquez vos résultats.\n",
    "\n",
    "3. Affichez les 10 centralités les plus élevées pour l'ensemble de données MCU. À titre indicatif, la centralité la plus élevée devrait être de 0.073.\n",
    "\n",
    "---\n",
    "\n",
    "A final simple measure to find the importance of a node in a network is the betweeness centrality. It is calculated $$C_{I}(i) = \\frac{\\sum_{j<k}f_{jk}(i)}{\\binom{n-1}{2}}$$\n",
    "\n",
    "- n: the number of nodes in the network\n",
    "- i: a node in the network\n",
    "- j,k: two nodes in the network excluding i\n",
    "- $f_{jk}(i)$: the proportion of shortest paths from vertex j to vertex k (> j) passing through node i\n",
    "\n",
    "#### Implementation\n",
    "1. Implement the function `calculate_betweenness_centrality`.This function calculates the betweenness centrality for all the nodes of the network and adds this measurement to each node.\n",
    "\n",
    "**NB**: Use the functions `all_shortest_paths()` or `shortest_path()` from Networkx to compute the shortest simple paths in the graph.\n",
    "\n",
    "2. The dataset has two connected components. Use the function `nx.connected_components` to get each one of them. Calculate the closeness centrality for each component, and explain your results. \n",
    "\n",
    "3. Show the highest 10 centralities for the MCU dataset. As a hint, the highest centrality should be 0.073."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LEWZX5kLDzDY"
   },
   "outputs": [],
   "source": [
    "from math import factorial\n",
    "\n",
    "def calculate_betweenness_centrality(social_network):\n",
    "    ## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvzPr6TEDzDY"
   },
   "source": [
    "# 3. Méchants MCU/MCU Villains (4 points)\n",
    "\n",
    "L'univers cinématographique Marvel est connu pour avoir des méchants très notables. Nous vous demandons de construire un modèle pour suivre ces méchants. Vos fonctionnalités doivent être chaque film dans lequel les personnages ont été et les mesures de centralité calculées auparavant.\n",
    "\n",
    "---\n",
    "\n",
    "The Marvel Cinematic Universe is known to have very notable villains. We ask you to build a model to track those villains. Your features should be each movie that the characters have been in, and the centrality measures calculated before. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JyTQV7pDzDY"
   },
   "source": [
    "## 3.1. Création du ensemble de données/Creating the dataset (2 points)\n",
    "\n",
    "À partir de la liste des films fournis, créez des features à partir de la colonne `Movie` afin d'entraîner votre jeu de données, et fusionner ces valeurs avec les centralités, pour chaque personnage. Utilisez la méthode de votre choix dans ce processus.\n",
    "\n",
    "Les centralités étant très variables, mieux vaut normaliser chaque mesure. Sélectionnez une méthode de votre choix pour ramener toutes les mesures à une seule échelle.\n",
    "\n",
    "Utilisez le fichier .CSV *`tp-marvel-villains.csv`* comme principale source d'indication si un personnage est un méchant. Fusionnez ce fichier dans l'ensemble de données global en tant qu'étiquette Mechant (la feature `y`).\n",
    "\n",
    "---\n",
    "\n",
    "From the list of movies given, create features from the column `Movie` so as to train your dataset, and merge these values ​​with the centralities, for each character.. Use any method of your choice in this process.\n",
    "\n",
    "As the centralities vary greatly, it is better to normalize each measure. Select a method of your choice to bring all of the measures to a single scale.  \n",
    "\n",
    "Use the .CSV file *`tp-marvel-villains.csv`* as the main source of indication whether a character is a villain. Merge this file into the overall dataset as the Villain label (the `y` feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cc1f-0xWDzDY"
   },
   "outputs": [],
   "source": [
    "## Mettez votre code ici\n",
    "\n",
    "## Insert your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jRa77VuDzDZ"
   },
   "source": [
    "## 3.2. Construire le modèle/Building the model (2 points)\n",
    "\n",
    "\n",
    "Maintenant, utilisez votre créativité pour construire un modèle pour prédire les méchants dans l'ensemble de données, à partir des fonctionnalités que vous avez créées. Voici quelques points pour vous guider :\n",
    "\n",
    "1. Des démonstrations graphiques seront appréciées. Ne vous contentez pas de prédire vos données, affichez vos valeurs de manière agréable.\n",
    "2. Expliquez vos résultats. L'ajout des fonctionnalités de centralité aide-t-il votre modèle d'une manière ou d'une autre? Les films aident-ils votre modèle? Si vous ne deviez garder qu'un seul des deux sous-ensembles, lequel garderiez-vous?\n",
    "\n",
    "---\n",
    "\n",
    "Now, use your creativity to build a model to predict the villains in the dataset, from the features that you have created. Here are some points to guide you:\n",
    "\n",
    "1. Graphical demonstrations will be appreciated. Do not just predict your data, show your values in a pleasant way. \n",
    "2. Explain your results. Does the addition of the centrality features help your model somehow? Do the movies help your model? If you had to keep just one of the two subsets, which one would you keep?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SH0RPEurDzDZ"
   },
   "outputs": [],
   "source": [
    "## Mettez votre code ici\n",
    "\n",
    "## Insert your code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "9df1e68bf46b73c6ebe6a76ad55019ef4844b94d3c6c2e70b2602d401fb2ac5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
